{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Transformers\r\n",
    "\r\n",
    "Will use the model implemented in [keras_pp_dense_forecasting notebook](https://github.com/CahidArda/internship-study/blob/main/notebooks/rnn/keras_pp_dense_forecasting.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "#import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "sys.path.append(r'../atm_demand')\r\n",
    "from feature_generation import *\r\n",
    "\r\n",
    "#from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing and Formatting Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "df = pd.read_csv(\"../atm_demand/DATA_sample_atm.csv\")\r\n",
    "targets = ['CashIn', 'CashOut']\r\n",
    "atm_df = get_atm(df, 26637)\r\n",
    "atm_df = atm_df[:-135]\r\n",
    "atm_df = clean_data(atm_df, drop_zeros=True)\r\n",
    "feature_set = get_feature_sets(atm_df, ['CashIn', 'CashOut'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "feature_set.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['CashIn', 'CashOut', 'CashIn_average_7', 'CashIn_average_30',\n",
       "       'CashOut_average_7', 'CashOut_average_30', 'CashIn_trend_7',\n",
       "       'CashOut_trend_7', 'CashIn_t-1', 'CashIn_t-2', 'CashIn_t-3',\n",
       "       'CashIn_t-4', 'CashIn_t-5', 'CashIn_t-6', 'CashIn_t-7', 'CashIn_t-8',\n",
       "       'CashIn_t-9', 'CashIn_t-10', 'CashIn_t-11', 'CashIn_t-12',\n",
       "       'CashIn_t-13', 'CashIn_t-14', 'CashOut_t-1', 'CashOut_t-2',\n",
       "       'CashOut_t-3', 'CashOut_t-4', 'CashOut_t-5', 'CashOut_t-6',\n",
       "       'CashOut_t-7', 'CashOut_t-8', 'CashOut_t-9', 'CashOut_t-10',\n",
       "       'CashOut_t-11', 'CashOut_t-12', 'CashOut_t-13', 'CashOut_t-14',\n",
       "       'CashOut_t-15', 'CashOut_t-16', 'CashOut_t-17', 'CashOut_t-18',\n",
       "       'CashOut_t-19', 'CashOut_t-20', 'CashOut_t-21', 'CashOut_t-22',\n",
       "       'CashOut_t-23', 'CashOut_t-24', 'CashOut_t-25', 'CashOut_t-26',\n",
       "       'CashOut_t-27', 'CashOut_t-28', 'CashOut_t-29', 'CashOut_t-30',\n",
       "       'CashOut_t-31', 'CashOut_t-32', 'CashOut_t-33', 'CashOut_t-34',\n",
       "       'CashOut_t-35', 'CashOut_t-36', 'CashOut_t-37', 'CashOut_t-38',\n",
       "       'CashOut_t-39', 'CashOut_t-40', 'Day_Of_the_Week_Index', 'Day_Index__0',\n",
       "       'Day_Index__1', 'Day_Index__2', 'Day_Index__3', 'Day_Index__4',\n",
       "       'Day_Index__5', 'Day_Index__6', 'Day_Of_the_Month_Index', 'Is_Weekday',\n",
       "       'Is_Weekend', 'curr_month_1_delta', 'curr_month_15_delta',\n",
       "       'next_month_1_delta', 'is_ramazan', 'ramazan_in_7_days', 'is_kurban',\n",
       "       'kurban_in_7_days'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "def get_input_sets(df, groups):\r\n",
    "    result = []\r\n",
    "    for group in groups:\r\n",
    "        result.append(df[group])\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "groups = []\r\n",
    "# Adding one hots\r\n",
    "groups.append('Is_Weekday, Is_Weekend, is_ramazan, ramazan_in_7_days, is_kurban, kurban_in_7_days'.split(', '))\r\n",
    "# Adding numericals\r\n",
    "groups.append(['CashIn_average_7', 'CashIn_average_30',\r\n",
    "       'CashOut_average_7', 'CashOut_average_30', 'CashIn_trend_7',\r\n",
    "       'CashOut_trend_7', 'CashIn_t-1', 'CashIn_t-2', 'CashIn_t-3',\r\n",
    "       'CashIn_t-4', 'CashIn_t-5', 'CashIn_t-6', 'CashIn_t-7', 'CashIn_t-8',\r\n",
    "       'CashIn_t-9', 'CashIn_t-10', 'CashIn_t-11', 'CashIn_t-12',\r\n",
    "       'CashIn_t-13', 'CashIn_t-14', 'CashOut_t-1', 'CashOut_t-2',\r\n",
    "       'CashOut_t-3', 'CashOut_t-4', 'CashOut_t-5', 'CashOut_t-6',\r\n",
    "       'CashOut_t-7', 'CashOut_t-8', 'CashOut_t-9', 'CashOut_t-10',\r\n",
    "       'CashOut_t-11', 'CashOut_t-12', 'CashOut_t-13', 'CashOut_t-14',\r\n",
    "       'CashOut_t-15', 'CashOut_t-16', 'CashOut_t-17', 'CashOut_t-18',\r\n",
    "       'CashOut_t-19', 'CashOut_t-20', 'CashOut_t-21', 'CashOut_t-22',\r\n",
    "       'CashOut_t-23', 'CashOut_t-24', 'CashOut_t-25', 'CashOut_t-26',\r\n",
    "       'CashOut_t-27', 'CashOut_t-28', 'CashOut_t-29', 'CashOut_t-30',\r\n",
    "       'CashOut_t-31', 'CashOut_t-32', 'CashOut_t-33', 'CashOut_t-34',\r\n",
    "       'CashOut_t-35', 'CashOut_t-36', 'CashOut_t-37', 'CashOut_t-38',\r\n",
    "       'CashOut_t-39', 'CashOut_t-40'])\r\n",
    "# Adding categoricals\r\n",
    "groups.append(['Day_Of_the_Week_Index'])\r\n",
    "groups.append(['Day_Of_the_Month_Index'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "def train_test_split(X, y, split=0.2):\r\n",
    "    cut = int(X.shape[0] * split)\r\n",
    "    return X[:-cut], X[-cut:], y[:-cut], y[-cut:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "TARGET = 'CashIn'\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_set[feature_set.columns[2:]], feature_set[TARGET])\r\n",
    "\r\n",
    "train_inputs = get_input_sets(X_train, groups)\r\n",
    "test_inputs  = get_input_sets(X_test, groups)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "one_hot_inputs = layers.Input(shape=[6])\r\n",
    "\r\n",
    "numeric_inputs        = layers.Input(shape=[60])\r\n",
    "numeric_inputs_normal = layers.LayerNormalization()(numeric_inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Categoricals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "EMBED_DIM = 16\r\n",
    "\r\n",
    "week_index_input    = layers.Input(shape=[1])\r\n",
    "week_index_embedded = layers.Embedding(input_dim=7, output_dim=EMBED_DIM)(week_index_input)\r\n",
    "\r\n",
    "month_index_input    = layers.Input(shape=[1])\r\n",
    "month_index_embedded = layers.Embedding(input_dim=31, output_dim=EMBED_DIM)(month_index_input)\r\n",
    "\r\n",
    "embeddeds = layers.Concatenate(axis=1)([week_index_embedded, month_index_embedded])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "class TransformerBlock(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n",
    "        super(TransformerBlock, self).__init__()\r\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.ffn = keras.Sequential(\r\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\r\n",
    "        )\r\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.dropout1 = layers.Dropout(rate)\r\n",
    "        self.dropout2 = layers.Dropout(rate)\r\n",
    "\r\n",
    "    def call(self, inputs, training):\r\n",
    "        attn_output = self.att(inputs, inputs)\r\n",
    "        attn_output = self.dropout1(attn_output, training=training)\r\n",
    "        out1 = self.layernorm1(inputs + attn_output)\r\n",
    "        ffn_output = self.ffn(out1)\r\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\r\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "HEADS   = 4\r\n",
    "DEPTH   = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "transformer = embeddeds\r\n",
    "for _ in range(DEPTH):\r\n",
    "    transformer = TransformerBlock(EMBED_DIM, HEADS, EMBED_DIM)(transformer)\r\n",
    "transformer_flat = layers.Flatten()(transformer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Concatenation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "concatenated = layers.Concatenate()([one_hot_inputs, numeric_inputs_normal, transformer_flat])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "'''\r\n",
    "l = concatenated.shape[1] // 8\r\n",
    "\r\n",
    "MLP_HIDDEN_RATIOS = [4,2,2]\r\n",
    "mlp_layer = concatenated\r\n",
    "for ratio in MLP_HIDDEN_RATIOS:\r\n",
    "    mlp_layer = layers.Dense(ratio * l, activation='relu')(mlp_layer)\r\n",
    "    mlp_layer = layers.Dropout(0.1)(mlp_layer)\r\n",
    "output_layer = layers.Dense(1)(mlp_layer)\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nl = concatenated.shape[1] // 8\\n\\nMLP_HIDDEN_RATIOS = [4,2,2]\\nmlp_layer = concatenated\\nfor ratio in MLP_HIDDEN_RATIOS:\\n    mlp_layer = layers.Dense(ratio * l, activation='relu')(mlp_layer)\\n    mlp_layer = layers.Dropout(0.1)(mlp_layer)\\noutput_layer = layers.Dense(1)(mlp_layer)\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "dropout1        = layers.Dropout(0.2)(concatenated)\r\n",
    "normalization1  = layers.LayerNormalization()(dropout1)\r\n",
    "dense1          = layers.Dense(128, activation='relu')(normalization1)\r\n",
    "dropout2        = layers.Dropout(0.2)(dense1)\r\n",
    "normalization2  = layers.LayerNormalization()(dropout2)\r\n",
    "dense2          = layers.Dense(32, activation='relu')(normalization2)\r\n",
    "dropout3        = layers.Dropout(0.2)(dense2)\r\n",
    "normalization3  = layers.LayerNormalization()(dropout3)\r\n",
    "dense3          = layers.Dense(16, activation='selu')(normalization3)\r\n",
    "exponential     = layers.Dense(16, activation='selu')(dense3)\r\n",
    "output_layer    = layers.Dense(1)(exponential)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "model = keras.Model(inputs=[one_hot_inputs, numeric_inputs, week_index_input, month_index_input], outputs=[output_layer])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 16)        112         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 16)        496         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 2, 16)        0           embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_30 (Transform (None, 2, 16)        4912        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_31 (Transform (None, 2, 16)        4912        transformer_block_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_32 (Transform (None, 2, 16)        4912        transformer_block_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_62 (LayerNo (None, 60)           120         input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 32)           0           transformer_block_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 98)           0           input_15[0][0]                   \n",
      "                                                                 layer_normalization_62[0][0]     \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 98)           0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_69 (LayerNo (None, 98)           196         dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 128)          12672       layer_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 128)          0           dense_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_70 (LayerNo (None, 128)          256         dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 32)           4128        layer_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 32)           0           dense_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_71 (LayerNo (None, 32)           64          dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 16)           528         layer_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_90 (Dense)                (None, 16)           272         dense_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 1)            17          dense_90[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 33,597\n",
      "Trainable params: 33,597\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "model.compile(\r\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.05),\r\n",
    "    loss='mape')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "history = model.fit(train_inputs, \r\n",
    "            y_train,\r\n",
    "            batch_size=32,\r\n",
    "            epochs=100,\r\n",
    "            validation_data=(test_inputs, y_test),\r\n",
    "            verbose = 1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 4s 20ms/step - loss: 98.7776 - val_loss: 66.0329\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 79.5364 - val_loss: 41.5176\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 74.7946 - val_loss: 40.8120\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 83.5098 - val_loss: 40.0720\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 59.0165 - val_loss: 40.1090\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 54.9802 - val_loss: 45.8443\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 75.1014 - val_loss: 41.1555\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 60.9866 - val_loss: 41.8973\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 53.6632 - val_loss: 43.3746\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 50.4393 - val_loss: 44.7823\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 56.5866 - val_loss: 45.6670\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 51.1304 - val_loss: 42.6037\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 50.0176 - val_loss: 50.9774\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 49.4943 - val_loss: 53.7743\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 45.1553 - val_loss: 47.5214\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 51.1897 - val_loss: 54.9150\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 47.9883 - val_loss: 56.1591\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 45.3875 - val_loss: 43.8636\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 58.2973 - val_loss: 53.1510\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 47.9277 - val_loss: 53.2472\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 44.6217 - val_loss: 44.9167\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 40.6830 - val_loss: 43.8081\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 44.7169 - val_loss: 49.7511\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 48.0089 - val_loss: 53.1573\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 46.4674 - val_loss: 40.5481\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 47.9851 - val_loss: 49.7693\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 46.4072 - val_loss: 51.5543\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 42.4452 - val_loss: 54.0368\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 42.5613 - val_loss: 57.3462\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 44.3665 - val_loss: 52.6488\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 43.0076 - val_loss: 50.4266\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 42.7126 - val_loss: 49.0493\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 49.1311 - val_loss: 44.3449\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 67.1104 - val_loss: 48.9501\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 45.1015 - val_loss: 43.8988\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 45.8586 - val_loss: 50.9450\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 59.4071 - val_loss: 54.9959\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 42.2868 - val_loss: 42.8762\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 43.6267 - val_loss: 46.6053\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 40.7643 - val_loss: 47.0696\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 43.0242 - val_loss: 54.5866\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 39.3946 - val_loss: 48.0712\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 40.4255 - val_loss: 46.7441\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 39.4171 - val_loss: 47.7396\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 39.2004 - val_loss: 44.2495\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 57.2105 - val_loss: 62.2295\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 51.1844 - val_loss: 59.4038\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 46.8470 - val_loss: 57.3797\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 44.2545 - val_loss: 49.7800\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 42.0514 - val_loss: 49.5615\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 41.3333 - val_loss: 49.5410\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 40.5157 - val_loss: 48.7955\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 41.3932 - val_loss: 50.6705\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 45.7147 - val_loss: 46.0187\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 38.5871 - val_loss: 48.1440\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 42.9618 - val_loss: 48.5331\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 41.2968 - val_loss: 49.2885\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 37.6859 - val_loss: 48.3644\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 38.9195 - val_loss: 42.7463\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 44.3637 - val_loss: 51.4316\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 41.0514 - val_loss: 55.3777\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 38.4462 - val_loss: 49.4470\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 36.2631 - val_loss: 42.9868\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 39.2249 - val_loss: 47.2117\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 40.6828 - val_loss: 45.2318\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 37.2053 - val_loss: 42.0162\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 41.4594 - val_loss: 44.1862\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 40.2233 - val_loss: 51.3812\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 41.0750 - val_loss: 46.9362\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 38.4099 - val_loss: 47.0914\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 39.2819 - val_loss: 46.3862\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 35.0994 - val_loss: 45.0178\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 38.6345 - val_loss: 41.6871\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 35.7005 - val_loss: 46.1625\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 36.8379 - val_loss: 48.4319\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 36.5926 - val_loss: 47.7175\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 38.1794 - val_loss: 49.3556\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 35.6816 - val_loss: 48.1425\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 38.3775 - val_loss: 52.3348\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 37.5356 - val_loss: 48.2107\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 34.8106 - val_loss: 46.1974\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 35.6402 - val_loss: 51.0371\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 35.6232 - val_loss: 46.4290\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 36.4437 - val_loss: 43.5276\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 37.7071 - val_loss: 43.0436\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 38.3830 - val_loss: 43.6511\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 36.2380 - val_loss: 43.2029\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 37.6803 - val_loss: 45.2016\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 37.4209 - val_loss: 46.5796\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 35.4255 - val_loss: 49.5282\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 34.8332 - val_loss: 44.8021\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 32.7268 - val_loss: 44.7759\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 35.6161 - val_loss: 45.0410\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 36.6967 - val_loss: 43.2839\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 33.1069 - val_loss: 42.2051\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 34.7748 - val_loss: 37.7830\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 35.6550 - val_loss: 47.4438\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 34.4988 - val_loss: 50.3300\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 35.9618 - val_loss: 47.0499\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 33.2656 - val_loss: 42.6952\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "\"min_loss: %.4f, min_val_loss: %.4f\" % (min(history.history['loss']), min(history.history['val_loss']))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'min_loss: 33.7116, min_val_loss: 37.7830'"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing with Previous Methods\r\n",
    "\r\n",
    "| Model | Train Error | Test Error | All Set Error |\r\n",
    "| - | - | - | - |\r\n",
    "| Base | - | - | 78.8 |\r\n",
    "| Random Forest | 23.9 | 47.0 | - |\r\n",
    "| LGBM | - | 48.7 | - |\r\n",
    "| TF Custom RNN | - | - | 70.7 |\r\n",
    "| Keras, PP + Dense (with one dropout layer) | 21.7 | 37.8 | - |\r\n",
    "| Keras, PP + Dense (with two dropout layers) | 25.3 | 36.9 | - |\r\n",
    "| Keras, PP + Dense [1] | 18.9 | 35.5 | - |\r\n",
    "| Keras, PP + Dense [2] | 18.5 | 34.7 | - |\r\n",
    "| Keras, PP + Transformer + Dense [2] | 33.7 | 37.8 | - |\r\n",
    "\r\n",
    "* [1]: dropout, layer-norm, dense (128, relu), dropout, layer-norm, dense (32, relu), dense (16, relu), dense (16, **selu**), dense (1)\r\n",
    "* [2]: dropout, layer-norm, dense (128, relu), dropout, layer-norm, dense (32, relu), dropout, layer-norm, dense (16, **selu**), dense (16, **selu**), dense (1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}