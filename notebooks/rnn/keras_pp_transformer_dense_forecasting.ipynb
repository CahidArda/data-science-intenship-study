{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Transformers\r\n",
    "\r\n",
    "This model is an attempt to implement the [TabTransformer](https://github.com/lucidrains/tab-transformer-pytorch).\r\n",
    "\r\n",
    "I will develop the model in [keras_pp_dense_forecasting notebook](https://github.com/CahidArda/internship-study/blob/main/notebooks/rnn/keras_pp_dense_forecasting.ipynb)\r\n",
    "\r\n",
    "Here is the diagram of the model:\r\n",
    "\r\n",
    "<img src=\"imgs/modelDiagram.png\">\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "#import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "sys.path.append(r'../atm_demand')\r\n",
    "from feature_generation import *\r\n",
    "\r\n",
    "#from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing and Formatting Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "df = pd.read_csv(\"../atm_demand/DATA_sample_atm.csv\")\r\n",
    "targets = ['CashIn', 'CashOut']\r\n",
    "atm_df = get_atm(df, 26637)\r\n",
    "atm_df = atm_df[:-135]\r\n",
    "atm_df = clean_data(atm_df, drop_zeros=True)\r\n",
    "feature_set = get_feature_sets(atm_df, ['CashIn', 'CashOut'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "special_dates = pd.Series(0, index = feature_set.index, name='Special_Dates_Index')\r\n",
    "for i, feature in enumerate(['is_ramazan', 'ramazan_in_7_days', 'is_kurban','kurban_in_7_days']):\r\n",
    "    special_dates[feature_set[feature] == 1] = i + 1\r\n",
    "\r\n",
    "feature_set[special_dates.name] = special_dates"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "feature_set.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['CashIn', 'CashOut', 'CashIn_average_7', 'CashIn_average_30',\n",
       "       'CashOut_average_7', 'CashOut_average_30', 'CashIn_trend_7',\n",
       "       'CashOut_trend_7', 'CashIn_t-1', 'CashIn_t-2', 'CashIn_t-3',\n",
       "       'CashIn_t-4', 'CashIn_t-5', 'CashIn_t-6', 'CashIn_t-7', 'CashIn_t-8',\n",
       "       'CashIn_t-9', 'CashIn_t-10', 'CashIn_t-11', 'CashIn_t-12',\n",
       "       'CashIn_t-13', 'CashIn_t-14', 'CashOut_t-1', 'CashOut_t-2',\n",
       "       'CashOut_t-3', 'CashOut_t-4', 'CashOut_t-5', 'CashOut_t-6',\n",
       "       'CashOut_t-7', 'CashOut_t-8', 'CashOut_t-9', 'CashOut_t-10',\n",
       "       'CashOut_t-11', 'CashOut_t-12', 'CashOut_t-13', 'CashOut_t-14',\n",
       "       'CashOut_t-15', 'CashOut_t-16', 'CashOut_t-17', 'CashOut_t-18',\n",
       "       'CashOut_t-19', 'CashOut_t-20', 'CashOut_t-21', 'CashOut_t-22',\n",
       "       'CashOut_t-23', 'CashOut_t-24', 'CashOut_t-25', 'CashOut_t-26',\n",
       "       'CashOut_t-27', 'CashOut_t-28', 'CashOut_t-29', 'CashOut_t-30',\n",
       "       'CashOut_t-31', 'CashOut_t-32', 'CashOut_t-33', 'CashOut_t-34',\n",
       "       'CashOut_t-35', 'CashOut_t-36', 'CashOut_t-37', 'CashOut_t-38',\n",
       "       'CashOut_t-39', 'CashOut_t-40', 'Day_of_the_Week_Index', 'Day_Index__0',\n",
       "       'Day_Index__1', 'Day_Index__2', 'Day_Index__3', 'Day_Index__4',\n",
       "       'Day_Index__5', 'Day_Index__6', 'Day_of_the_Month_Index',\n",
       "       'Week_of_the_Year_Index', 'Is_Weekday', 'Is_Weekend',\n",
       "       'curr_month_1_delta', 'curr_month_15_delta', 'next_month_1_delta',\n",
       "       'is_ramazan', 'ramazan_in_7_days', 'is_kurban', 'kurban_in_7_days',\n",
       "       'Special_Dates_Index'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "one_hots     = []\r\n",
    "categoricals = ['Day_of_the_Week_Index', 'Day_of_the_Month_Index', 'Week_of_the_Year_Index', 'Special_Dates_Index', 'Is_Weekday']\r\n",
    "numericals   = ['CashIn_average_7', 'CashIn_average_30',\r\n",
    "       'CashOut_average_7', 'CashOut_average_30', 'CashIn_trend_7',\r\n",
    "       'CashOut_trend_7', 'CashIn_t-1', 'CashIn_t-2', 'CashIn_t-3',\r\n",
    "       'CashIn_t-4', 'CashIn_t-5', 'CashIn_t-6', 'CashIn_t-7', 'CashIn_t-8',\r\n",
    "       'CashIn_t-9', 'CashIn_t-10', 'CashIn_t-11', 'CashIn_t-12',\r\n",
    "       'CashIn_t-13', 'CashIn_t-14', 'CashOut_t-1', 'CashOut_t-2',\r\n",
    "       'CashOut_t-3', 'CashOut_t-4', 'CashOut_t-5', 'CashOut_t-6',\r\n",
    "       'CashOut_t-7', 'CashOut_t-8', 'CashOut_t-9', 'CashOut_t-10',\r\n",
    "       'CashOut_t-11', 'CashOut_t-12', 'CashOut_t-13', 'CashOut_t-14',\r\n",
    "       'CashOut_t-15', 'CashOut_t-16', 'CashOut_t-17', 'CashOut_t-18',\r\n",
    "       'CashOut_t-19', 'CashOut_t-20', 'CashOut_t-21', 'CashOut_t-22',\r\n",
    "       'CashOut_t-23', 'CashOut_t-24', 'CashOut_t-25', 'CashOut_t-26',\r\n",
    "       'CashOut_t-27', 'CashOut_t-28', 'CashOut_t-29', 'CashOut_t-30',\r\n",
    "       'CashOut_t-31', 'CashOut_t-32', 'CashOut_t-33', 'CashOut_t-34',\r\n",
    "       'CashOut_t-35', 'CashOut_t-36', 'CashOut_t-37', 'CashOut_t-38',\r\n",
    "       'CashOut_t-39', 'CashOut_t-40']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def train_test_split(X, y, split=0.2):\r\n",
    "    cut = int(X.shape[0] * split)\r\n",
    "    return X[:-cut], X[-cut:], y[:-cut], y[-cut:]\r\n",
    "\r\n",
    "def get_input_sets(df, groups):\r\n",
    "    result = []\r\n",
    "    for group in groups:\r\n",
    "        result.append(df[group])\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "TARGET = 'CashIn'\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_set[feature_set.columns[2:]], feature_set[TARGET])\r\n",
    "\r\n",
    "groups = [one_hots, numericals]\r\n",
    "groups.extend(categoricals)\r\n",
    "train_inputs = get_input_sets(X_train, groups)\r\n",
    "test_inputs  = get_input_sets(X_test, groups)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "one_hot_inputs = layers.Input(shape=[len(one_hots)])\r\n",
    "\r\n",
    "numeric_inputs        = layers.Input(shape=[len(numericals)])\r\n",
    "numeric_inputs_normal = layers.LayerNormalization()(numeric_inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Categoricals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "EMBED_DIM = 16\r\n",
    "\r\n",
    "categorical_inputs = []\r\n",
    "embedding_outputs  = []\r\n",
    "for size in [len(feature_set[categorical].unique()) for categorical in categoricals]:\r\n",
    "    input_layer          = layers.Input(shape=[1])\r\n",
    "    input_layer_embedded = layers.Embedding(input_dim=size, output_dim=EMBED_DIM)(input_layer)\r\n",
    "    categorical_inputs.append(input_layer)\r\n",
    "    embedding_outputs.append(input_layer_embedded)\r\n",
    "\r\n",
    "embeddeds = layers.Concatenate(axis=1)(embedding_outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "class TransformerBlock(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n",
    "        super(TransformerBlock, self).__init__()\r\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.ffn = keras.Sequential(\r\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\r\n",
    "        )\r\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.dropout1 = layers.Dropout(rate)\r\n",
    "        self.dropout2 = layers.Dropout(rate)\r\n",
    "\r\n",
    "    def call(self, inputs, training):\r\n",
    "        attn_output = self.att(inputs, inputs)\r\n",
    "        attn_output = self.dropout1(attn_output, training=training)\r\n",
    "        out1 = self.layernorm1(inputs + attn_output)\r\n",
    "        ffn_output = self.ffn(out1)\r\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\r\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "HEADS   = 8\r\n",
    "DEPTH   = 6"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "transformer = embeddeds\r\n",
    "for _ in range(DEPTH):\r\n",
    "    transformer = TransformerBlock(EMBED_DIM, HEADS, EMBED_DIM)(transformer)\r\n",
    "transformer_flat = layers.Flatten()(transformer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Concatenation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "concatenated = layers.Concatenate()([one_hot_inputs, numeric_inputs_normal, transformer_flat])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "'''\r\n",
    "l = concatenated.shape[1] // 8\r\n",
    "\r\n",
    "MLP_HIDDEN_RATIOS = [4,2]\r\n",
    "mlp_layer = concatenated\r\n",
    "for ratio in MLP_HIDDEN_RATIOS:\r\n",
    "    mlp_layer = layers.Dropout(0.05)(mlp_layer)\r\n",
    "    mlp_layer = layers.LayerNormalization()(mlp_layer)\r\n",
    "    mlp_layer = layers.Dense(ratio * l, activation='relu')(mlp_layer)\r\n",
    "output_layer = layers.Dense(1)(mlp_layer)\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nl = concatenated.shape[1] // 8\\n\\nMLP_HIDDEN_RATIOS = [4,2]\\nmlp_layer = concatenated\\nfor ratio in MLP_HIDDEN_RATIOS:\\n    mlp_layer = layers.Dropout(0.05)(mlp_layer)\\n    mlp_layer = layers.LayerNormalization()(mlp_layer)\\n    mlp_layer = layers.Dense(ratio * l, activation='relu')(mlp_layer)\\noutput_layer = layers.Dense(1)(mlp_layer)\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "dropout1        = layers.Dropout(0.2)(concatenated)\r\n",
    "normalization1  = layers.LayerNormalization()(dropout1)\r\n",
    "dense1          = layers.Dense(128, activation='relu')(normalization1)\r\n",
    "dropout2        = layers.Dropout(0.2)(dense1)\r\n",
    "normalization2  = layers.LayerNormalization()(dropout2)\r\n",
    "dense2          = layers.Dense(32, activation='relu')(normalization2)\r\n",
    "dropout3        = layers.Dropout(0.2)(dense2)\r\n",
    "normalization3  = layers.LayerNormalization()(dropout3)\r\n",
    "dense3          = layers.Dense(16, activation='selu')(normalization3)\r\n",
    "exponential     = layers.Dense(16, activation='selu')(dense3)\r\n",
    "output_layer    = layers.Dense(1)(exponential)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "inputs = [one_hot_inputs, numeric_inputs]\r\n",
    "inputs.extend(categorical_inputs)\r\n",
    "model = keras.Model(inputs=inputs, outputs=[output_layer])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 16)        112         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 16)        496         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 16)        864         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 16)        80          input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 16)        32          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 5, 16)        0           embedding_5[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_6 (Transforme (None, 5, 16)        9200        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_7 (Transforme (None, 5, 16)        9200        transformer_block_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_8 (Transforme (None, 5, 16)        9200        transformer_block_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_9 (Transforme (None, 5, 16)        9200        transformer_block_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_10 (Transform (None, 5, 16)        9200        transformer_block_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_11 (Transform (None, 5, 16)        9200        transformer_block_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 0)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 60)           120         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 80)           0           transformer_block_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 140)          0           input_10[0][0]                   \n",
      "                                                                 layer_normalization_16[0][0]     \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 140)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 140)          280         dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 128)          18048       layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 128)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 128)          256         dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 32)           4128        layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 32)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 32)           64          dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 16)           528         layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 16)           272         dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1)            17          dense_30[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 80,497\n",
      "Trainable params: 80,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "model.compile(\r\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.05),\r\n",
    "    loss='mape')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "history = model.fit(train_inputs, \r\n",
    "            y_train,\r\n",
    "            batch_size=32,\r\n",
    "            epochs=100,\r\n",
    "            validation_data=(test_inputs, y_test),\r\n",
    "            verbose = 1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 7s 37ms/step - loss: 97.8414 - val_loss: 41.6359\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 68.6743 - val_loss: 39.8237\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 57.0911 - val_loss: 41.2892\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 66.1903 - val_loss: 39.7251\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 81.6277 - val_loss: 42.3300\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 92.0212 - val_loss: 43.1256\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 64.8195 - val_loss: 40.6053\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 73.8418 - val_loss: 40.1723\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 53.2627 - val_loss: 40.4151\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 63.4173 - val_loss: 40.5197\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 94.3061 - val_loss: 41.6988\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 56.1096 - val_loss: 42.6219\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 68.1130 - val_loss: 39.6588\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 82.8036 - val_loss: 42.9764\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 59.8136 - val_loss: 39.8832\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 53.3741 - val_loss: 39.5311\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 90.6481 - val_loss: 39.8419\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 69.3693 - val_loss: 39.6486\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 54.9705 - val_loss: 39.7302\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 58.3148 - val_loss: 39.6457\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 73.8099 - val_loss: 39.9381\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 73.8241 - val_loss: 40.5044\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 57.6535 - val_loss: 40.1008\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 75.5268 - val_loss: 39.9217\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 101.3064 - val_loss: 46.7753\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 73.7109 - val_loss: 39.8166\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 55.5493 - val_loss: 39.4428\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 57.4347 - val_loss: 40.1377\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 57.7788 - val_loss: 38.6184\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 83.3214 - val_loss: 43.5383\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 70.0514 - val_loss: 39.8334\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 53.5472 - val_loss: 39.8842\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 100.9891 - val_loss: 39.5291\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 69.8167 - val_loss: 38.5221\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 67.6890 - val_loss: 38.3750\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 70.8673 - val_loss: 38.9170\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 52.4223 - val_loss: 40.3833\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 70.9446 - val_loss: 41.7550\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 63.1361 - val_loss: 38.8929\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 61.1177 - val_loss: 39.9915\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 64.8843 - val_loss: 39.6784\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 55.9882 - val_loss: 39.2888\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 51.5568 - val_loss: 37.4990\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 77.4081 - val_loss: 42.8939\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 82.7100 - val_loss: 44.9327\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 81.0330 - val_loss: 41.9883\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 53.6958 - val_loss: 37.8836\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 48.3841 - val_loss: 37.2080\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 115.4324 - val_loss: 44.8713\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 61.4303 - val_loss: 38.4274\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 50.5697 - val_loss: 37.9800\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 52.1012 - val_loss: 38.1032\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 54.6662 - val_loss: 45.4485\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 51.6235 - val_loss: 41.8079\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 47.9073 - val_loss: 41.7930\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 43.5541 - val_loss: 42.2179\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 42.8354 - val_loss: 43.7134\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 59.1057 - val_loss: 46.9146\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 49.6256 - val_loss: 45.6259\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 46.5183 - val_loss: 43.6997\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 40.8908 - val_loss: 45.7951\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 45.8332 - val_loss: 49.9894\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 43.9367 - val_loss: 43.9575\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 42.2639 - val_loss: 44.1684\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 42.6166 - val_loss: 52.7283\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 41.8440 - val_loss: 49.3734\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 41.1539 - val_loss: 47.8534\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 39.4511 - val_loss: 46.1283\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 40.7255 - val_loss: 46.2986\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 40.5293 - val_loss: 49.3848\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 42.5697 - val_loss: 50.4714\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 39.4868 - val_loss: 46.0732\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 38.6477 - val_loss: 41.9377\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 40.9856 - val_loss: 50.4947\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 41.7936 - val_loss: 46.7815\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 48.8816 - val_loss: 56.7358\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 42.5940 - val_loss: 51.9794\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 40.6473 - val_loss: 46.0294\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 43.8733 - val_loss: 49.7240\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 42.7974 - val_loss: 47.9402\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 43.0981 - val_loss: 50.2688\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 43.0748 - val_loss: 46.1904\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 38.9048 - val_loss: 43.2661\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 39.0215 - val_loss: 47.9707\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 40.7675 - val_loss: 46.4824\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 42.0812 - val_loss: 44.5707\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 39.1094 - val_loss: 42.9392\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 37.6429 - val_loss: 43.6393\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 38.5347 - val_loss: 49.3568\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 38.9882 - val_loss: 51.8097\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 40.2203 - val_loss: 43.0426\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 36.8998 - val_loss: 45.1760\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 39.2865 - val_loss: 41.6650\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 40.8958 - val_loss: 54.6179\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 39.0070 - val_loss: 45.1426\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 35.0620 - val_loss: 46.9973\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 38.7126 - val_loss: 43.8177\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 38.7432 - val_loss: 48.0236\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 38.5579 - val_loss: 43.4467\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 37.3902 - val_loss: 42.0177\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "\"min_loss: %.4f, min_val_loss: %.4f\" % (min(history.history['loss']), min(history.history['val_loss']))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'min_loss: 37.1741, min_val_loss: 37.2080'"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing with Previous Methods\r\n",
    "\r\n",
    "| Model | Train Error | Test Error | All Set Error |\r\n",
    "| - | - | - | - |\r\n",
    "| Base | - | - | 78.8 |\r\n",
    "| Random Forest | 23.9 | 47.0 | - |\r\n",
    "| LGBM | - | 48.7 | - |\r\n",
    "| TF Custom RNN | - | - | 70.7 |\r\n",
    "| Keras, PP + Dense (with one dropout layer) | 21.7 | 37.8 | - |\r\n",
    "| Keras, PP + Dense (with two dropout layers) | 25.3 | 36.9 | - |\r\n",
    "| Keras, PP + Dense [1] | 18.9 | 35.5 | - |\r\n",
    "| Keras, PP + Dense [2] | 18.5 | 34.7 | - |\r\n",
    "| Keras, PP + Transformer + Dense [2] | 33.2 | 37.2 | - |\r\n",
    "\r\n",
    "* [1]: dropout, layer-norm, dense (128, relu), dropout, layer-norm, dense (32, relu), dense (16, relu), dense (16, **selu**), dense (1)\r\n",
    "* [2]: dropout, layer-norm, dense (128, relu), dropout, layer-norm, dense (32, relu), dropout, layer-norm, dense (16, **selu**), dense (16, **selu**), dense (1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}